# Zenyatta Environment Variables
# Copy to ~/zenyatta/.env and fill in as needed (setup.sh does this for you)
# These are passed to the container and available to opencode, claude, etc.

# =============================================================================
# ANTHROPIC (Claude Code, opencode with Claude)
# =============================================================================
ANTHROPIC_API_KEY=

# =============================================================================
# OPENAI (ChatGPT, GPT-4, opencode with OpenAI)
# =============================================================================
OPENAI_API_KEY=
# Optional: Override base URL for OpenAI-compatible APIs
# OPENAI_API_BASE=https://api.openai.com/v1

# =============================================================================
# NVIDIA NIM
# =============================================================================
NVIDIA_API_KEY=

# =============================================================================
# GOOGLE (Gemini, opencode with Google)
# =============================================================================
GOOGLE_API_KEY=
# Or use Vertex AI:
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json

# =============================================================================
# GROQ (fast inference)
# =============================================================================
GROQ_API_KEY=

# =============================================================================
# TOGETHER AI
# =============================================================================
TOGETHER_API_KEY=

# =============================================================================
# MISTRAL
# =============================================================================
MISTRAL_API_KEY=

# =============================================================================
# AZURE OPENAI
# =============================================================================
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=
# AZURE_OPENAI_API_VERSION=2024-02-15-preview

# =============================================================================
# OLLAMA (local LLM server)
# =============================================================================
# Local network server (e.g., NAS, dedicated machine):
# OLLAMA_HOST=http://192.168.1.xxx:11434
#
# Same machine (Docker):  OLLAMA_HOST=http://host.docker.internal:11434
# Same machine (Podman):  OLLAMA_HOST=http://host.containers.internal:11434
# WSL2 to Windows host:   OLLAMA_HOST=http://$(cat /etc/resolv.conf | grep nameserver | awk '{print $2}'):11434
OLLAMA_HOST=

# =============================================================================
# X11 DISPLAY (usually auto-detected, override if needed)
# =============================================================================
# Native Linux: Leave empty (uses $DISPLAY from host)
# WSL2 + VcXsrv: DISPLAY=<your-windows-ip>:0
# WSLg (Win11): Leave empty (uses :0)
# DISPLAY=

# =============================================================================
# MCP (Model Context Protocol) - NO CONFIG NEEDED
# =============================================================================
# Claude Code MCP works out of the box. The container can reach:
# - Anthropic API (api.anthropic.com)
# - Any MCP servers you configure in Claude Code
# - Local network services (192.168.x.x, etc.)
#
# Security note: Dropped capabilities (NET_RAW, NET_ADMIN) only block raw
# sockets and firewall changes - standard HTTP/HTTPS/WebSocket is unaffected.
